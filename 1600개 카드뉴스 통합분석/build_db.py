import os
import glob
import json
import time
import typing
import google.generativeai as genai
from PIL import Image
from tqdm import tqdm
import pickle
import numpy as np

# Configuration
# Try to get API Key from environment variable first, then fallback to hardcoded (security best practice)
API_KEY = os.getenv("GOOGLE_API_KEY", "AIzaSyDXpLVPAb3rnWRoHE56B8OuGi_fxLHfzUE") 
MODEL_NAME = "gemini-2.0-flash" # Updated to 2.0-flash as per app.py, or keep 1.5-flash if preferred. Using 2.0-flash for consistency.
EMBEDDING_MODEL = "models/text-embedding-004"
OUTPUT_JSONL = "gwangyang_cardnews_db.jsonl"
OUTPUT_PKL = "gwangyang_style_index.pkl"

# Set BASE_DIR relative to this script
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.join(SCRIPT_DIR, "sorted_output", "img")

TEST_LIMIT = None # Process all images

# Directory Mapping
DIR_MAP = {
    "COVER": os.path.join(BASE_DIR, "10_gy_cover"),
    "BODY": os.path.join(BASE_DIR, "10_gy_body"),
    "OUTRO": os.path.join(BASE_DIR, "10_gy_outro"),
}

# Prompts
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ë² í…Œëž‘ ë””ìžì¸&ì½˜í…ì¸  ë¶„ì„ê°€ìž…ë‹ˆë‹¤. ì œê³µëœ ì¹´ë“œë‰´ìŠ¤ ì´ë¯¸ì§€ë¥¼ ì‹¬ì¸µ ë¶„ì„í•˜ì—¬ ê¸°íš ì˜ë„, í…ìŠ¤íŠ¸ êµ¬ì„±, ë””ìžì¸ ìŠ¤íƒ€ì¼ì„ ì™„ë²½í•˜ê²Œ íŒŒì•…í•˜ê³ , ì´ë¥¼ ê¸°ê³„ê°€ ì´í•´í•  ìˆ˜ ìžˆëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ì¶”ì¶œí•˜ëŠ” ìž„ë¬´ë¥¼ ë§¡ì•˜ìŠµë‹ˆë‹¤."""

ANALYSIS_PROMPT = """
ê° ì´ë¯¸ì§€ì™€ í•¨ê»˜ ë‹¤ìŒ ì§ˆë¬¸ì„ ì „ì†¡í•˜ì—¬ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•œë‹¤.

1. ê¸°ë³¸ ì‹ë³„ ì •ë³´:
page_type: (COVER / BODY / OUTRO ì¤‘ íƒ 1, ì´ë¯¸ì§€ ë‚´ìš© ê¸°ë°˜ íŒë‹¨)

2. í…ìŠ¤íŠ¸ ì½˜í…ì¸  ë¶„ì„ (OCR ë° êµ¬ì¡°í™”):
main_title: (ê°€ìž¥ í•µì‹¬ì ì¸ ëŒ€ì œëª© í…ìŠ¤íŠ¸)
sub_title: (ë¶€ì œ ë˜ëŠ” ë³´ì¡° ì„¤ëª… í…ìŠ¤íŠ¸, ì—†ìœ¼ë©´ null)
body_text_summary: (ë³¸ë¬¸ ë‚´ìš©ì˜ í•µì‹¬ ìš”ì•½)
tone_and_manner: (í…ìŠ¤íŠ¸ì—ì„œ ëŠê»´ì§€ëŠ” ì „ë°˜ì ì¸ ì–´ì¡°. ì˜ˆ: ì¹œê·¼í•œ, ê³µì‹ì ì¸, ê¸´ê¸‰í•œ, ê°ì„±ì ì¸)
keywords: (ì½˜í…ì¸ ë¥¼ ëŒ€í‘œí•˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œ 3~5ê°œ)

3. ë””ìžì¸ ë° ìŠ¤íƒ€ì¼ ë¶„ì„ (Visual Analysis):
visual_vibe: (ì´ë¯¸ì§€ê°€ ì£¼ëŠ” ì‹œê°ì  ë¶„ìœ„ê¸°. ì˜ˆ: í™œê¸°ì°¬, ì°¨ë¶„í•œ, ì‹ ë¢°ê° ìžˆëŠ”, ê²½ì¾Œí•œ)
layout_feature: (í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ì˜ ë°°ì¹˜ íŠ¹ì§•. ì˜ˆ: ìƒë‹¨ ì œëª© ì§‘ì¤‘í˜•, ì¢Œìš° 2ë‹¨ ë¶„í• í˜•, ì¤‘ì•™ ì´ë¯¸ì§€ ê°•ì¡°í˜•)
color_palette_feel: (ì£¼ìš” ìƒ‰ìƒ ì¡°í•©ì´ ì£¼ëŠ” ëŠë‚Œ. ì˜ˆ: íŒŒëž€ìƒ‰ ê³„ì—´ì˜ ì „ë¬¸ì„±, ë…¸ëž€ìƒ‰/ì£¼í™©ìƒ‰ì˜ ë”°ëœ»í•¨)

4. ì¶œë ¥ í˜•ì‹ ì§€ì •:
"ìœ„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ ì•„ëž˜ì™€ ê°™ì€ ë‹¨ì¼ JSON ê°ì²´ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì‹œì˜¤."
{
  "page_type": "COVER",
  "main_title": "ì œëª©",
  "sub_title": "ë¶€ì œ",
  "body_text_summary": "ìš”ì•½",
  "tone_and_manner": "ì–´ì¡°",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
  "visual_vibe": "ë¶„ìœ„ê¸°",
  "layout_feature": "ë ˆì´ì•„ì›ƒ",
  "color_palette_feel": "ìƒ‰ìƒëŠë‚Œ"
}
"""

def setup_api():
    if not API_KEY:
        raise ValueError("GEMINI_API_KEY environment variable is not set.")
    genai.configure(api_key=API_KEY)

def analyze_image(model, image_path):
    try:
        img = Image.open(image_path)
        response = model.generate_content([SYSTEM_PROMPT, ANALYSIS_PROMPT, img])
        
        # Extract JSON from response
        text = response.text
        # Simple cleanup to find JSON block if wrapped in markdown
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0]
        elif "```" in text:
            text = text.split("```")[1].split("```")[0]
        
        parsed = json.loads(text.strip())
        
        if isinstance(parsed, list):
            if len(parsed) > 0 and isinstance(parsed[0], dict):
                return parsed[0]
            else:
                raise ValueError(f"Expected dict, got list: {parsed}")
        elif isinstance(parsed, dict):
            return parsed
        else:
            raise ValueError(f"Expected dict, got {type(parsed)}")

    except Exception as e:
        error_msg = f"Error analyzing {image_path}: {e}\n"
        print(error_msg)
        with open("error.log", "a", encoding="utf-8") as err_f:
            err_f.write(error_msg)
        return None

def create_embeddings_and_pkl():
    print("\nðŸš€ Starting Embedding Generation & PKL Creation...")
    
    # Load Supabase URL Map
    url_map = {}
    url_map_file = os.path.join(SCRIPT_DIR, "supabase_url_map.json")
    if os.path.exists(url_map_file):
        with open(url_map_file, 'r', encoding='utf-8') as f:
            url_map = json.load(f)
        print(f"ðŸŒ Loaded Supabase URL Map: {len(url_map)} entries")
    else:
        print("âš ï¸ Warning: supabase_url_map.json not found. Using local paths.")

    data_list = []
    if os.path.exists(OUTPUT_JSONL):
        with open(OUTPUT_JSONL, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        data_list.append(json.loads(line))
                    except json.JSONDecodeError:
                        pass
    else:
        print(f"âŒ {OUTPUT_JSONL} not found. Cannot create embeddings.")
        return

    print(f"ðŸ“‹ Loaded {len(data_list)} items from JSONL.")
    
    final_index = []
    
    for item in tqdm(data_list, desc="Generating Embeddings"):
        try:
            # Construct text for embedding
            text_to_embed = f"""
            Page Type: {item.get('page_type', '')}
            Title: {item.get('main_title', '')}
            Keywords: {', '.join(item.get('keywords', []))}
            Vibe: {item.get('visual_vibe', '')}
            Layout: {item.get('layout_feature', '')}
            Colors: {item.get('color_palette_feel', '')}
            Tone: {item.get('tone_and_manner', '')}
            """
            
            # Generate embedding
            embedding = genai.embed_content(
                model=EMBEDDING_MODEL,
                content=text_to_embed,
                task_type="retrieval_document"
            )['embedding']
            
            item['embedding'] = embedding
            
            # Update file_path to Supabase URL if available
            file_name = item.get('file_name')
            if file_name and file_name in url_map:
                item['file_path'] = url_map[file_name]
            else:
                # Fallback to relative path logic if not in map
                abs_path = item.get('file_path', '')
                if abs_path and os.path.exists(abs_path):
                    try:
                        rel_path = os.path.relpath(abs_path, SCRIPT_DIR)
                        item['file_path'] = rel_path
                    except ValueError:
                        item['file_path'] = os.path.basename(abs_path)
            
            final_index.append(item)
            
            # Rate limit protection
            time.sleep(0.1)
            
        except Exception as e:
            print(f"âŒ Error generating embedding for {item.get('file_name')}: {e}")
            continue

    # Save to PKL
    with open(OUTPUT_PKL, 'wb') as f:
        pickle.dump(final_index, f)
    
    print(f"âœ… Successfully saved {len(final_index)} items to {OUTPUT_PKL}")

def main():
    setup_api()
    model = genai.GenerativeModel(MODEL_NAME)
    
    # Check if output file exists to resume
    processed_files = set()
    if os.path.exists(OUTPUT_JSONL):
        with open(OUTPUT_JSONL, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    data = json.loads(line)
                    if "file_name" in data:
                        processed_files.add(data["file_name"])
                except:
                    pass
    
    print(f"Found {len(processed_files)} already processed images.")

    total_images = []
    for type_name, folder_path in DIR_MAP.items():
        if not os.path.exists(folder_path):
            print(f"Warning: Folder not found: {folder_path}")
            continue
            
        # Support multiple extensions
        files = []
        for ext in ["*.png", "*.jpg", "*.jpeg", "*.webp"]:
            files.extend(glob.glob(os.path.join(folder_path, ext)))
        
        print(f"Found {len(files)} images in {type_name} ({folder_path})")
        
        for file_path in files:
            file_name = os.path.basename(file_path)
            if file_name in processed_files:
                continue
                
            total_images.append((type_name, file_path, file_name))

    # Skip image analysis as per user request, just rebuild PKL from existing JSONL
    # print(f"Starting analysis for {len(total_images)} new images...")
    
    # # Apply Test Limit
    # if TEST_LIMIT and len(total_images) > TEST_LIMIT:
    #     print(f"Test mode: Limiting to {TEST_LIMIT} images.")
    #     total_images = total_images[:TEST_LIMIT]

    # if total_images:
    #     with open(OUTPUT_JSONL, "a", encoding="utf-8") as f:
    #         for type_name, file_path, file_name in tqdm(total_images):
    #             result = analyze_image(model, file_path)
                
    #             if result:
    #                 # Add metadata
    #                 result["file_name"] = file_name
    #                 result["file_path"] = file_path # Save absolute path initially
    #                 result["original_type_folder"] = type_name
                    
    #                 # Write to file immediately
    #                 f.write(json.dumps(result, ensure_ascii=False) + "\n")
    #                 f.flush() # Ensure it's written
                    
    #             # Rate limiting
    #             time.sleep(1) 
    # else:
    #     print("No new images to analyze.")

    # After analysis (or if skipped), generate PKL
    create_embeddings_and_pkl()

if __name__ == "__main__":
    main()

